{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I had every intention to prepare for this technical talk.\n",
    "> Sadly, that didn't work out because I faced these challenges.\n",
    "> I was caught up in a heavy workload, and didn't have time to think about anything.\n",
    ">  -- The Excuse Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Text Generation using GPT-2\n",
    "We'll look at using the huggingface *transformers* library to generate text and perform some other natural language processing tasks.\n",
    "\n",
    "The culmination of this NLP extravaganza will be a versitile, tireless, endlessly creative **Excuse Generator** powered by Artificial Intelligence.\n",
    "\n",
    "```python\n",
    "goal = 'write a better summary for this document'\n",
    "tasks = ['contemplate the most important ideas', 'determine the most efficient way to summarize the ideas',\n",
    "\t 'summarize ways to apply to industry', 'discuss next steps']\n",
    "s = ExcuseSituation(gen, assignment=goal, tasks=tasks, is_team=False, blame_others=False)\n",
    "s.generate_excuses(count=1)\n",
    "```\n",
    "\n",
    "> I was supposed to write a better summary for this document.\n",
    "> Things were going smoothly, but the problems started when I began to contemplate the most important ideas.\n",
    "> I wanted to do this, but I couldn't because I would have to spend hours writing down all the details. \n",
    "-- The Excuse Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [github.com/huggingface/transformers](https://github.com/huggingface/transformers#quick-tour)\n",
    "* [Understanding GPT-2](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)\n",
    "* [Transformer Neural Network Architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) (Google Blog)\n",
    "* [OpenAI: Better Language Models](https://openai.com/blog/better-language-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generative Pre-trained [Transformer](http://jalammar.github.io/illustrated-transformer/) from [OpenAI](https://openai.com/)\n",
    "* Trained on 40GB of text from 8 million web pages\n",
    "* Goal: predict the next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of what GPT-2 can do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [OpenAI Blog](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "**Given the prompt:**\n",
    "> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    "**GPT-2 Wrote**\n",
    ">The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    ">Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    ">Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    ">Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    ">Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n",
    "\n",
    ">While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n",
    "\n",
    ">Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n",
    "\n",
    ">While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n",
    "\n",
    ">However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the sample that's on the transformers website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "text = \"Who was Jim Henson ? Jim Henson was a\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who was Jim Henson? Jim Henson was a man'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# get the predicted next sub-word (in our case, the word 'man')\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "\n",
    "predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self):\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "        # Set the model in evaluation mode to deactivate the DropOut modules\n",
    "        # This is IMPORTANT to have reproducible results during evaluation!\n",
    "        self.model.eval()\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        self.model.to('cuda')\n",
    "\n",
    "    \n",
    "    def generate_word(self, start_text):\n",
    "        \"\"\"\n",
    "        Generate one word (or sub-word, sometimes) to add onto some start text.\n",
    "        The generated word will contain a leader space if appropriate.\n",
    "        \"\"\"\n",
    "        # Encode text inputs\n",
    "        indexed_tokens = self.tokenizer.encode(start_text)\n",
    "        # Convert indexed tokens in a PyTorch tensor\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        # Move the tokens to the GPU.\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        \n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        # get the predicted next sub-word.\n",
    "        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "        generated_word = self.tokenizer.decode(predicted_index)\n",
    "        return generated_word\n",
    "    \n",
    "    def generate(self, start_text, word_count=7):\n",
    "        text = start_text\n",
    "        for _ in range(word_count):\n",
    "            text += self.generate_word(text)\n",
    "        return text\n",
    "    \n",
    "    def generate_sentence(self, start_text, sentence_count=1, up_to_count = None):\n",
    "        if up_to_count:\n",
    "            sentence_count = random.randint(1, up_to_count)\n",
    "        text = start_text\n",
    "        sentence = ''\n",
    "        for _ in range(sentence_count):\n",
    "            while (True):\n",
    "                word = self.generate_word(text)\n",
    "                text += word\n",
    "                sentence += word\n",
    "                if '.' in word:\n",
    "                    break\n",
    "        return sentence\n",
    "\n",
    "gen = TextGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' far'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate_word(\"Can't stop now; I've travelled so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can't stop now; I've travelled a lot. I've been to a lot of\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate(\"Can't stop now; I've travelled\",  10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My assignment was to wash the dishes.  I couldn't do that because I wasn't sure if the water was warm enough.  If it was warm enough, the water\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = gen.generate(\"My assignment was to wash the dishes.  I couldn't do that because\",20)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My assignment was to wash the dishes.  I couldn't do that because I was too busy.               I feel terrible about that because I didn't do it. \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text += \"I feel terrible about that because\"\n",
    "gen.generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlameModeMe:\n",
    "    my = 'my'\n",
    "    i = 'I'\n",
    "    me = 'me'\n",
    "\n",
    "class BlameModeYou:\n",
    "    my = 'your'\n",
    "    i = 'you'\n",
    "    me = 'you'\n",
    "\n",
    "class BlameModeTeam:\n",
    "    my = 'our'\n",
    "    i = 'we'\n",
    "    me = 'us'\n",
    "\n",
    "class BlameModeTeamBlameShift:\n",
    "    my = 'their'\n",
    "    i = 'they'\n",
    "    me = 'them'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTRO_TEXT = [\n",
    "    '[my] assignment was to',\n",
    "    '[I] was supposed to',\n",
    "    '[I] intended to',\n",
    "    '[my] goal was to',\n",
    "    '[my] dream, [my] destiny was to',\n",
    "    '[I] had every intention to'\n",
    "]\n",
    "\n",
    "HOWEVER_TEXT = [\n",
    "    \"Sadly, that didn't work out because\",\n",
    "    \"Unfortunately, there was a serious problem with that\",\n",
    "    \"[I] couldn't do that because\",\n",
    "]\n",
    "\n",
    "TASK_INTRO = [\n",
    "    'Things were going pretty well until [I] got to the part where [I] needed to',\n",
    "    '[I] hit a serious problem when [I] started to',\n",
    "    '[I] hit a major roadblock when [I] began to',\n",
    "    'It was going fine until [I] went to',\n",
    "    'Things were going smoothly, but the problems started when I began to',\n",
    "]\n",
    "\n",
    "TASK_TRANSITION = [\n",
    "    'The problem was',\n",
    "    'What stopped [me] dead',\n",
    "    'Where [I] ran into a roadblock was',\n",
    "    \"[I] couldn't finish this because\",\n",
    "    \"[I] was unable to start this because\",\n",
    "    \"[I] wanted to do this, but I couldn't because\",\n",
    "    \"[I] started on that, but couldn't finish because\",\n",
    "]\n",
    "\n",
    "EMOTION_TURNAROUND = [\n",
    "    '[I] feel pretty sad about this because',\n",
    "    \"[I] hope you're not too disappointed and\",\n",
    "    \"On the bright side,\",\n",
    "    \"We shouldn't worry about it because\",\n",
    "    \"But the real problem to focus on here is\",\n",
    "    \"[I] am concerned about this because\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unfortunately, there was a serious problem with that'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(HOWEVER_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You know that we would never do that'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'You know that [I] would never do that'.replace('[I]', 'we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcuseSituation:\n",
    "    def __init__(self, text_generator, assignment, tasks=[], is_team=False, blame_others=False):\n",
    "        self.assignment = assignment\n",
    "        self.tasks = tasks\n",
    "        self.generator = text_generator\n",
    "        if is_team:\n",
    "            if blame_others:\n",
    "                mode = BlameModeTeamBlameShift\n",
    "            else:\n",
    "                mode = BlameModeTeam\n",
    "        else:\n",
    "            if blame_others:\n",
    "                mode = BlameModeYou\n",
    "            else:\n",
    "                mode = BlameModeMe\n",
    "        self.mode = mode\n",
    "    \n",
    "    def generate_excuse(self):\n",
    "        if random.random() < 0.5 or not self.tasks:\n",
    "            text = self.generate_excuse_whole()\n",
    "        else:\n",
    "            text = self.generate_excuse_task()\n",
    "        if random.random() < 0.5:\n",
    "            text += random.choice(EMOTION_TURNAROUND)\n",
    "            more = self.generator.generate_sentence(text)\n",
    "            text = self._list_to_text([text, more])\n",
    "        return text\n",
    "        \n",
    "    def generate_excuse_task(self):\n",
    "        background = [random.choice(INTRO_TEXT), self.assignment, '.']\n",
    "        background += [random.choice(TASK_INTRO), random.choice(self.tasks), '.']\n",
    "        background += [random.choice(TASK_TRANSITION)]\n",
    "        background_text = self._list_to_text(background)\n",
    "        text = self.generator.generate_sentence(background_text, up_to_count=3)\n",
    "        return self._prep_result(background_text + text)\n",
    "        \n",
    "    def generate_excuse_whole(self):\n",
    "        # Lead-in text, setting up the situation.\n",
    "        background = [random.choice(INTRO_TEXT), self.assignment, '.']\n",
    "        background += [random.choice(HOWEVER_TEXT)]\n",
    "        \n",
    "        background_text = self._list_to_text(background)\n",
    "        text = self.generator.generate_sentence(background_text)\n",
    "        return self._prep_result(background_text + text)\n",
    "\n",
    "    def generate_excuses(self, count=5):\n",
    "        result = []\n",
    "        for _ in range(count):\n",
    "            result.append(self.generate_excuse())\n",
    "        return result\n",
    "            \n",
    "    def _list_to_text(self, chunk_list):\n",
    "        words = []\n",
    "        for entry in chunk_list:\n",
    "            entry = entry.replace('[I]', self.mode.i)\n",
    "            entry = entry.replace('[my]', self.mode.my)\n",
    "            entry = entry.replace('[me]', self.mode.me)\n",
    "            words.append(entry)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def _prep_result(self, excuse_text):\n",
    "        return excuse_text[0].upper() + excuse_text[1:] # Capitalize sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I was supposed to prepare a nice dinner for you for Valentine's day . I couldn't do that because I was too busy with my work and my family.I hope you're not too disappointed and  I hope you're not too disappointed.\",\n",
       " \"My goal was to prepare a nice dinner for you for Valentine's day . Unfortunately, there was a serious problem with that.But the real problem to focus on here is  that I was not able to get the recipe to work.\",\n",
       " \"My goal was to prepare a nice dinner for you for Valentine's day . I hit a major roadblock when I began to plan the menu . I wanted to do this, but I couldn't because I was too busy with work and my family. I had to make a decision. I wanted to make a meal that was good for you and that was good for me.We shouldn't worry about it because  we're all in this together.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ExcuseSituation(gen, assignment=\"prepare a nice dinner for you for Valentine's day\", tasks=[\n",
    "    'plan the menu',\n",
    "    'go to the grocery store to buy the ingredients',\n",
    "    'cook it up',\n",
    "    'plate the meal in an attractive way',\n",
    "])\n",
    "s.generate_excuses(count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fight the Repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there's a lot of repetition in the text. For example:\n",
    "* I was supposed to prepare a nice dinner for you for Valentine's day . I couldn't do that because I was too busy with my work and my family. **I hope you're not too disappointed and  I hope you're not too disappointed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GPT2 Issue: Repetition](https://github.com/huggingface/transformers/issues/1725).  Solution is to use *temperarture*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new TextGenerator with a temperature parameter to shake things up a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self):\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        # Temperature adds randomness and unpredictability.\n",
    "        self.temperature = random.choice([0.8, 0.8, 0.8, 0.9, 0.7])\n",
    "\n",
    "        # Set the model in evaluation mode to deactivate the DropOut modules\n",
    "        # This is IMPORTANT to have reproducible results during evaluation!\n",
    "        self.model.eval()\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        self.model.to('cuda')\n",
    "\n",
    "    \n",
    "    def generate_word(self, start_text):\n",
    "        \"\"\"\n",
    "        Generate one word (or sub-word, sometimes) to add onto some start text.\n",
    "        The generated word will contain a leader space if appropriate.\n",
    "        \"\"\"\n",
    "        # Encode text inputs\n",
    "        indexed_tokens = self.tokenizer.encode(start_text)\n",
    "        # Convert indexed tokens in a PyTorch tensor\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        # Move the tokens to the GPU.\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        \n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor)\n",
    "            # Use temperature to add more chaos into the selection of words.\n",
    "            next_token_logits = outputs[0][:, -1, :] / self.temperature   # <--- Temperature\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1) # <-- Temperature\n",
    "\n",
    "        # get the predicted next sub-word.\n",
    "        generated_word = self.tokenizer.decode(next_token)\n",
    "        return generated_word\n",
    "    \n",
    "    def generate(self, start_text, word_count=7):\n",
    "        text = start_text\n",
    "        for _ in range(word_count):\n",
    "            text += self.generate_word(text)\n",
    "        return text\n",
    "    \n",
    "    def generate_sentence(self, start_text, sentence_count=1, up_to_count = None):\n",
    "        if up_to_count:\n",
    "            sentence_count = random.randint(1, up_to_count)\n",
    "        sentence_count=2 # TEMPORARY\n",
    "        text = start_text\n",
    "        sentence = ''\n",
    "        for _ in range(sentence_count):\n",
    "            while (True):\n",
    "                word = self.generate_word(text)\n",
    "                text += word\n",
    "                sentence += word\n",
    "                if '.' in word:\n",
    "                    break\n",
    "        return sentence\n",
    "\n",
    "gen = TextGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I intended to prepare a nice dinner for you for Valentine's day . It was going fine until I went to go to the grocery store to buy the ingredients . I started on that, but couldn't finish because I was having problems with the grocery store. It was really hard to get some ingredients on hand.\",\n",
       " \"I intended to prepare a nice dinner for you for Valentine's day . I hit a major roadblock when I began to plate the meal in an attractive way . I couldn't finish this because the waiter was too busy helping me out with a small bill. I apologize in advance for it being too late.\",\n",
       " \"I was supposed to prepare a nice dinner for you for Valentine's day . Sadly, that didn't work out because you were late. I was hoping to get a fancy meal before Valentine's Day so I could go to bed and hang out with my friend.\",\n",
       " \"My dream, my destiny was to prepare a nice dinner for you for Valentine's day . Unfortunately, there was a serious problem with that, and I was forced to wait till the next day to get a bite. I was afraid that I may get caught up in the exchange of glances and kisses from your friends in the restaurant.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ExcuseSituation(gen, assignment=\"prepare a nice dinner for you for Valentine's day\", tasks=[\n",
    "    'plan the menu',\n",
    "    'go to the grocery store to buy the ingredients',\n",
    "    'cook it up',\n",
    "    'plate the meal in an attractive way',\n",
    "])\n",
    "s.generate_excuses(count=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crazy stuff this came up with:\n",
    "* My dream, my destiny was to prepare a nice dinner for you for Valentine\\'s day . I hit a major roadblock when I began to plan the menu . What stopped me dead in my tracks?\" What stopped me dead in my tracks?\" \"You know what, **I want you to live inside my house for a while**, so you can **enjoy my calm and wayward, hard-working, and true love**.\"\n",
    "* I was supposed to prepare a nice dinner for you for Valentine's day . Unfortunately, there was a serious problem with that dinner. My girlfriend thought I was **acting out as a man** so I had to be careful, but it turned out that there was a real problem.\n",
    "* My assignment was to prepare a nice dinner for you for Valentine's day . Things were going pretty well until I got to the part where I needed to go to the grocery store to buy the ingredients . The problem was, I knew **there was already so much food in the fridge**. I had to look around to find the ingredients I needed and it was only over a month later when I got home that **I found the ingredient list for Trader Joe's frozen food**.\n",
    "* I had every intention to prepare a nice dinner for you for Valentine's day . I hit a major roadblock when I began to plan the menu . What stopped me dead in my tracks was my own faraway world of extremities. **How could I conclude that food would be a primary way to trigger deep emotional cycles of desire?**\n",
    "* My goal was to prepare a nice dinner for you for Valentine\\'s day . I hit a serious problem when I started to go to the grocery store to buy the ingredients . The problem was that I had no idea how to prepare it. **I set aside my grocery money and said to myself, \"where have I seen this** and how can I prove to myself how to prepare it with my own money?\" **Now buy some flowers and you can actually have a good time.**\n",
    "* My dream, my destiny was to prepare a nice dinner for you for Valentine's day . Sadly, that didn't work out because **I had wasted my money and we both spent a lot**. Therefore, **I did all of my annual shopping for my daughter, and it has been a pleasure to watch her grow into a confident, confident young man.**\n",
    "* I had every intention to prepare a nice dinner for you for Valentine's day . Sadly, that didn't work out because my ass was still sore and I was already too sad and hungry to eat. So I decided to come up with an excuse to drive home, but then, having just finished my first breakfast, I ran into my friend, who just happened to be the new president of the United States.\n",
    "*  **I was unable to start this because I had 42 different kinds of rice dishes. I was supposed to try each dish individually to see what would appeal to you the most.**\n",
    "* Things we going smoothly, but the problems started when I began to plate the meal in an attractive way. I was unable to start this because I had to sit back for a couple of days, because with my Master's degree.\\n\\n I was so busy with waiting for the lunch that I wasn't doing the dishes properly. I feel pretty sad about this because  **I am very sick**, so it was hard for me to take care of this situation. **I was diagnosed with Cervus Neovis, which is a histolytic disease that affects bone health**.\n",
    "* I intended to prepare a nice dinner for you for Valentine's day . It was going fine until I went to cook it up . I was unable to start this because I was starving but I had to before I could eat it.\\n I made it in half.I feel pretty sad about this because I am going to make money and not have to take care of this. **I am gonna die soon, so I will be out of work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you're thinking about using GPT2 to generate text for children or business, you might want to put another model on there to classify if the output is appropriate.  Or maybe fine tune it a bit on your own corpus, and tune the temperature for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I had every intention to make sure the model generates text that is appropriate for the business, not outputting anything too offensive or racy . Things were going smoothly, but the problems started when I began to tune the chaos parameter . What stopped me dead was the fact that the model had an impressive range of measurements on the right side of the high-frequency recording, which might be interpreted as arbitrary, but what actually changed was the way the middle of the record was measured. The precision scale was different for the middle and the top end of the recording and for the different degrees of concave concave.',\n",
       " \"I was supposed to make sure the model generates text that is appropriate for the business, not outputting anything too offensive or racy . Sadly, that didn't work out because the model did not include the real output of the other models. So, we had a problem in the one that was tested with 200 models.\",\n",
       " 'My dream, my destiny was to make sure the model generates text that is appropriate for the business, not outputting anything too offensive or racy . I hit a serious problem when I started to test the results on real children . Where I ran into a roadblock was the fact that it appeared to me that the page I was using was biased towards politically correct language. I understood that writing specifically about a political issue, even with the aim of showcasing the content on social media would be controversial, because of the very subtle contextual biases that were causing the language to be biased.',\n",
       " 'I intended to make sure the model generates text that is appropriate for the business, not outputting anything too offensive or racy . Unfortunately, there was a serious problem with that. I thought I would take the time to write a post about some of those issues explaining the technical aspects of using an AI to acquire and use illegal porn.On the bright side,  I was also able to do a little testing with an AI of some kind to see how it would perform on something like that. The results were surprising.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ExcuseSituation(gen, assignment=\"make sure the model generates text that is appropriate for the business, not outputting anything too offensive or racy\", tasks=[\n",
    "    'tune the chaos parameter',\n",
    "    'classify the outputs with another model',\n",
    "    'test the results on real children',\n",
    "    'run it through a bad word filter',\n",
    "])\n",
    "s.generate_excuses(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was supposed to finish that report and analysis . It was going fine until I went to talk to the customers . I couldn\\'t finish this because I couldn\\'t talk to them, they wouldn\\'t talk with me, which was the only way I could get my information.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Now, for all the data you\\'ve gathered, what\\'s arguably the most interesting area is the sort of particularity you have in the varied cognitive abilities of people with ADHD (dementia).On the bright side,  you can also find more reasons to be interested in what cognitive abilities might explain something like this. The questions you asked were:\\n\\n \\'Why does learning and language differ across different people with ADHD and schizophrenia?\\'\\n\\nThey were very clear as to what the reasons were.',\n",
       " 'My dream, my destiny was to finish that report and analysis . Things were going pretty well until I got to the part where I needed to do a detailed write-up of the analysis . What stopped me dead in my tracks was an update on the AI. I wanted to see how the AI was able to get back to the previous world.On the bright side,  while I was waiting for the report, I was able to analyze the AI. I found that it was very similar to what we see in the games we see today.',\n",
       " \"I intended to finish that report and analysis . I hit a major roadblock when I began to talk to the customers . I couldn't finish this because my business depended on it.\\n Yet, one day I got my first response to the query on the page.\",\n",
       " \"My dream, my destiny was to finish that report and analysis . Things were going pretty well until I got to the part where I needed to run the numbers . The problem was I had no idea how to get through the list and manage this list once I thought about it. I didn't even have a clue what to do with the numbers one way or another.\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ExcuseSituation(gen, assignment='finish that report and analysis',\n",
    "                    tasks=['talk to the customers', 'write up the customer feedback',\n",
    "                          'run the numbers', 'do a detailed write-up of the analysis'])\n",
    "s.generate_excuses(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My dream, my destiny was to prepare for this technical talk . Things were going pretty well until I got to the part where I needed to practice the presentation . I couldn't finish this because of the lack of students, but I wanted to finish it and I was already in the main hall. I took my seat and I pretended to be a student, but people could see my pupils just like I did.I feel pretty sad about this because  this was my very first time in the media (and I only graduated 12 years ago). I didn't know many students from another major, so I didn't really know how to function or how to be helpful to people.\",\n",
       " \"I was supposed to prepare for this technical talk . Things were going smoothly, but the problems started when I began to practice the presentation . I wanted to do this, but I couldn't because of my age. I felt bad about it, but I didn't want to stay at home and let my Japanese friends call me and ask me questions.But the real problem to focus on here is  that I was human, Obama said. I was just a kid who had amazing friends.\",\n",
       " \"My goal was to prepare for this technical talk . Things were going pretty well until I got to the part where I needed to create the presentation slides . I wanted to do this, but I couldn't because I didn't know how to use a printer in a computer. So I thought it would be cool to dig into the slides and get my hands dirty by doing something like this.I feel pretty sad about this because  I'll never be able to do this with a computer. However, I did like the presentation slides because it revealed what I was really thinking about when designing the presentation slides.\",\n",
       " \"I had every intention to prepare for this technical talk . Sadly, that didn't work out because I couldn't get my hands on a working copy of the book. When I finally got it printed, I wrote it down and compared it with a Preface on adding another scene in the movie and that's how I ended up creating a chapter of the book.We shouldn't worry about it because  the book is already up for review, but it seems like our review count is on the wane because I don't really want to write a book about this. We will start to get back to it because I'm sure we will hit another milestone.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal = 'prepare for this technical talk'\n",
    "tasks = ['consider various ideas for things to discuss', 'prepare a prototype',\n",
    "         'create the presentation slides', 'write up the results', 'practice the presentation']\n",
    "s = ExcuseSituation(gen, assignment=goal, tasks=tasks, is_team=False, blame_others=False)\n",
    "s.generate_excuses(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I had every intention to put away the laundry . I couldn't do that because I knew the crowd would be too packed. So I grabbed my belt and stepped in front of them.\",\n",
       " 'I intended to put away the laundry . Unfortunately, there was a serious problem with that it prevented me to get into the house.\\n I had to cut the bathroom, and it is not installed correctly.',\n",
       " 'My dream, my destiny was to put away the laundry . It was going fine until I went to folding it . I was unable to start this because it was getting too cold so I went to my neighbor\\'s, and he was kind enough to let me go. When I got there, he had cleaned my bed and put it on its side, and I just sat there and thought \"oh no, that\\'s not going to work out right now, I\\'m going to sleep in the closet, I\\'m going to die in the closet,\" and then he ran out the door and, \"I\\'ll tell you what, I\\'m going to get my name off the street and I\\'m going to take care of my own life,\" and then I was gone, and that\\'s when I started to realize I was a freak.',\n",
       " \"I was supposed to put away the laundry . Unfortunately, there was a serious problem with that.\\n I went out and did some laundry on it.I feel pretty sad about this because  I really don't want someone to ruin my day.\\n\\n It's been a long day, but I'm really happy with the results.\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal = 'put away the laundry'\n",
    "tasks = ['getting it out of the dryer', 'folding it',\n",
    "         'putting it on hangers', 'putting it in its specified location',\n",
    "        'getting off the couch']\n",
    "s = ExcuseSituation(gen, assignment=goal, tasks=tasks, is_team=False, blame_others=False)\n",
    "s.generate_excuses(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My assignment was to write a better summary for this document . It was going fine until I went to determine the most efficient way to summarize the ideas . Where I ran into a roadblock was that of my editor. I was confused by the amount of time I had to explain to her what the idea of writing a better summary was.',\n",
       " \"My goal was to write a better summary for this document . I hit a serious problem when I started to determine the most efficient way to summarize the ideas . I was unable to start this because I had already written a bit of things in the past. I think a lot of people use the memoizer as a way of organizing their code so all their comments will need to be written in an exact same way.But the real problem to focus on here is  that the outline of the document is covered so much in detail that you don't even know what the actual contents are. I decided to write just this portion of the document because I needed to expand this document a bit.\",\n",
       " \"I was supposed to write a better summary for this document . Things were going smoothly, but the problems started when I began to contemplate the most important ideas . I wanted to do this, but I couldn't because I would have to spend hours writing down all the details. Now, I had a whole lot of details in my head, but only a few in hand.\",\n",
       " 'My assignment was to write a better summary for this document . Sadly, that didn\\'t work out because I was tired of having to come up with a better way of talking about other interesting papers. Also, there\\'s a lot of overlap between the \"new\" information on my work and the \"old\" information, and I\\'m probably not going to be able to break it out into any useful pieces, which is why it\\'s so important to note that I\\'m also not going to be able to write a new book on this specific topic.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal = 'write a better summary for this document'\n",
    "tasks = ['contemplate the most important ideas', 'determine the most efficient way to summarize the ideas',\n",
    "]\n",
    "s = ExcuseSituation(gen, assignment=goal, tasks=tasks, is_team=False, blame_others=False)\n",
    "s.generate_excuses(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
